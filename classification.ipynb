{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66408313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [Imports]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f738e7",
   "metadata": {},
   "source": [
    "# Classification Analysis\n",
    "\n",
    "This notebook covers classification using Decision Trees (ID3, CART), Naive Bayes, and Support Vector Machine (SVM). Model evaluation and comparison are included for each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7387ffc",
   "metadata": {},
   "source": [
    "## 1. Data Loading, Preprocessing, and Exploration\n",
    "\n",
    "We will load the dataset, preprocess it to optimize classification performance, and visualize key aspects. Preprocessing includes handling missing values, encoding categorical variables, and scaling features. These steps are chosen to ensure models receive clean, numerical, and standardized data, which improves accuracy and comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ded7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración global\n",
    "RANDOM_STATE = 42\n",
    "rng = check_random_state(RANDOM_STATE)\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# Tamaños para trabajar de forma eficiente en máquinas normales\n",
    "TEST_SIZE = 0.2\n",
    "N_SPLITS = 3\n",
    "SAMPLE_SIZE_FOR_GRID = 50_000   # filas para tuning (GridSearch)\n",
    "SAMPLE_SIZE_FOR_SVM  = 20_000   # filas para entrenar SVM RBF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26abea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº muestras: 999999 | Nº features candidatas: 14\n",
      "Distribución de clases (top 8):\n",
      " Genre\n",
      "Drama          0.25\n",
      "Comedy         0.20\n",
      "Action         0.15\n",
      "Thriller       0.10\n",
      "Romance        0.10\n",
      "Horror         0.10\n",
      "Documentary    0.05\n",
      "Sci-Fi         0.05\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [Data loading, pre-processing & exploration]\n",
    "DATA_PATH = \"./data/movies_dataset.csv\"  # ajusta si es otra ruta/archivo\n",
    "TARGET = \"Genre\"\n",
    "\n",
    "# Carga\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Parseo de fecha (día-mes-año en tu CSV)\n",
    "df[\"ReleaseDate\"] = pd.to_datetime(df[\"ReleaseDate\"], dayfirst=True, errors=\"coerce\")\n",
    "df[\"ReleaseMonth\"]   = df[\"ReleaseDate\"].dt.month\n",
    "df[\"ReleaseDay\"]     = df[\"ReleaseDate\"].dt.day\n",
    "df[\"ReleaseWeekday\"] = df[\"ReleaseDate\"].dt.weekday\n",
    "\n",
    "# Selección de variables con baja/mediana cardinalidad (evitamos One-Hot gigante)\n",
    "features_num = [\n",
    "    \"ReleaseYear\", \"ReleaseMonth\", \"ReleaseDay\", \"ReleaseWeekday\",\n",
    "    \"BudgetUSD\", \"US_BoxOfficeUSD\", \"Global_BoxOfficeUSD\",\n",
    "    \"Opening_Day_SalesUSD\", \"One_Week_SalesUSD\",\n",
    "    \"IMDbRating\", \"RottenTomatoesScore\", \"NumVotesIMDb\", \"NumVotesRT\"\n",
    "]\n",
    "features_cat = [\"Country\"]  # One-Hot controlado\n",
    "\n",
    "X_full = df[features_num + features_cat].copy()\n",
    "y_full = df[TARGET].copy()\n",
    "\n",
    "# Exploración mínima\n",
    "print(f\"Nº muestras: {len(df)} | Nº features candidatas: {len(features_num) + len(features_cat)}\")\n",
    "print(\"Distribución de clases (top 8):\\n\", y_full.value_counts(normalize=True).round(3))\n",
    "\n",
    "# Utilidad: submuestreo estratificado por clase\n",
    "def stratified_sample(X, y, n_per_class):\n",
    "    parts = []\n",
    "    for cls in y.unique():\n",
    "        idx = y[y == cls].index\n",
    "        take = min(n_per_class, len(idx))\n",
    "        sampled = rng.choice(idx, size=take, replace=False)\n",
    "        parts.append(pd.DataFrame({\"i\": sampled}))\n",
    "    sel = pd.concat(parts)[\"i\"].values\n",
    "    return X.loc[sel], y.loc[sel]\n",
    "\n",
    "# Generamos un conjunto manejable para grids (aprox. 50k)\n",
    "n_per_class_grid = SAMPLE_SIZE_FOR_GRID // y_full.nunique()\n",
    "X_small, y_small = stratified_sample(X_full, y_full, n_per_class_grid)\n",
    "\n",
    "# Split train/test estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small, test_size=TEST_SIZE, stratify=y_small, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Preprocesadores: StandardScaler (num) + One-Hot (cat)\n",
    "numeric_std = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "numeric_minmax = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "categorical_oh = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "prep_std = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_std, features_num),\n",
    "        (\"cat\", categorical_oh, features_cat)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "prep_minmax = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_minmax, features_num),\n",
    "        (\"cat\", categorical_oh, features_cat)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Función de evaluación común\n",
    "def evaluate_model(nombre, modelo, X_te, y_te):\n",
    "    y_pred = modelo.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_te, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n== {nombre} ==\")\n",
    "    print(pd.Series({\"accuracy\": acc, \"precision_macro\": pr, \"recall_macro\": rc, \"f1_macro\": f1}))\n",
    "    print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
    "    print(\"\\nReporte por clase:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
    "    return {\"model\": nombre, \"accuracy\": acc, \"precision_macro\": pr, \"recall_macro\": rc, \"f1_macro\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04aea68",
   "metadata": {},
   "source": [
    "## 3. Decision Trees (ID3 & CART)\n",
    "\n",
    "We will train and evaluate Decision Tree classifiers using both ID3 (entropy) and CART (gini) criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2c64d",
   "metadata": {},
   "source": [
    "ID3 (entropía)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fef3240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Después de tu preprocessing, simplemente llama:\\n\\n# Sin transformación (ID3 hará la discretización internamente)\\nid3_model = train_id3_model(X_train, X_test, y_train, y_test, max_depth=8, n_bins=5)\\n\\n# Para ver el árbol (cuidado si es muy grande):\\n# id3_model.print_tree()\\n\\n# O si quieres usar evaluate_model de tu código:\\nfrom sklearn.pipeline import Pipeline\\n\\n# Crea un pipeline vacío (ID3 no necesita preprocessing adicional)\\nid3_pipeline = Pipeline([\\n    (\\'id3\\', ID3DecisionTree(max_depth=8, min_samples_split=10, n_bins=5))\\n])\\n\\nid3_pipeline.fit(X_train, y_train)\\nevaluate_model(\"ID3 Decision Tree\", id3_pipeline, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "class ID3DecisionTree(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Implementación del algoritmo ID3 (Iterative Dichotomiser 3)\n",
    "    para árboles de decisión con discretización automática de variables numéricas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, n_bins=5):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        max_depth : int, opcional\n",
    "            Profundidad máxima del árbol\n",
    "        min_samples_split : int, default=2\n",
    "            Mínimo de muestras requeridas para dividir un nodo\n",
    "        n_bins : int, default=5\n",
    "            Número de bins para discretizar variables numéricas\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_bins = n_bins\n",
    "        self.tree_ = None\n",
    "        self.feature_names_ = None\n",
    "        self.discretizer_ = None\n",
    "        \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calcula la entropía de un conjunto de etiquetas\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        counts = Counter(y)\n",
    "        probs = np.array(list(counts.values())) / len(y)\n",
    "        return -np.sum(probs * np.log2(probs + 1e-10))\n",
    "    \n",
    "    def _information_gain(self, X_column, y, threshold=None):\n",
    "        \"\"\"Calcula la ganancia de información para un atributo\"\"\"\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        # Para atributos categóricos (discretizados)\n",
    "        if threshold is None:\n",
    "            values = np.unique(X_column)\n",
    "            weighted_entropy = 0\n",
    "            for val in values:\n",
    "                mask = X_column == val\n",
    "                if np.sum(mask) > 0:\n",
    "                    subset_entropy = self._entropy(y[mask])\n",
    "                    weighted_entropy += (np.sum(mask) / len(y)) * subset_entropy\n",
    "        else:\n",
    "            # Para split binario (si lo necesitas)\n",
    "            left_mask = X_column <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            left_weight = np.sum(left_mask) / len(y)\n",
    "            right_weight = np.sum(right_mask) / len(y)\n",
    "            \n",
    "            weighted_entropy = (left_weight * self._entropy(y[left_mask]) +\n",
    "                              right_weight * self._entropy(y[right_mask]))\n",
    "        \n",
    "        return parent_entropy - weighted_entropy\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Encuentra el mejor atributo para dividir\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            gain = self._information_gain(X[:, feature_idx], y)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature_idx\n",
    "        \n",
    "        return best_feature, best_gain\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Construye el árbol de decisión recursivamente\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Debug cada ciertos niveles\n",
    "        if depth <= 3:\n",
    "            print(f\"[DEBUG] Profundidad {depth}: {n_samples} muestras, {n_classes} clases\")\n",
    "        \n",
    "        # Condiciones de parada\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           n_classes == 1 or \\\n",
    "           n_samples < self.min_samples_split:\n",
    "            # Nodo hoja: retorna la clase más común\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return {\"leaf\": True, \"value\": leaf_value, \"samples\": n_samples}\n",
    "        \n",
    "        # Encuentra el mejor split\n",
    "        best_feature, best_gain = self._best_split(X, y)\n",
    "        \n",
    "        if best_gain == 0:\n",
    "            # No hay ganancia de información\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return {\"leaf\": True, \"value\": leaf_value, \"samples\": n_samples}\n",
    "        \n",
    "        # Crea ramas para cada valor del atributo\n",
    "        feature_values = np.unique(X[:, best_feature])\n",
    "        branches = {}\n",
    "        \n",
    "        for value in feature_values:\n",
    "            mask = X[:, best_feature] == value\n",
    "            if np.sum(mask) > 0:\n",
    "                branches[value] = self._build_tree(X[mask], y[mask], depth + 1)\n",
    "        \n",
    "        return {\n",
    "            \"leaf\": False,\n",
    "            \"feature\": best_feature,\n",
    "            \"feature_name\": self.feature_names_[best_feature] if self.feature_names_ else best_feature,\n",
    "            \"branches\": branches,\n",
    "            \"samples\": n_samples\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el árbol ID3\"\"\"\n",
    "        # Convierte a numpy array si es necesario\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_ = X.columns.tolist()\n",
    "            X = X.values\n",
    "        else:\n",
    "            self.feature_names_ = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        \n",
    "        # Discretiza variables numéricas\n",
    "        # Usa subsample para datasets grandes (más rápido y evita errores)\n",
    "        subsample_size = min(200000, X.shape[0])\n",
    "        \n",
    "        self.discretizer_ = KBinsDiscretizer(\n",
    "            n_bins=self.n_bins, \n",
    "            encode='ordinal', \n",
    "            strategy='quantile',\n",
    "            subsample=subsample_size\n",
    "        )\n",
    "        \n",
    "        print(f\"[DEBUG] Discretizando {X.shape[1]} features con {self.n_bins} bins...\")\n",
    "        X_discrete = self.discretizer_.fit_transform(X)\n",
    "        print(f\"[DEBUG] Discretización completada. Shape: {X_discrete.shape}\")\n",
    "        \n",
    "        # Construye el árbol\n",
    "        self.tree_ = self._build_tree(X_discrete, y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _predict_sample(self, x, tree):\n",
    "        \"\"\"Predice la clase para una muestra\"\"\"\n",
    "        if tree[\"leaf\"]:\n",
    "            return tree[\"value\"]\n",
    "        \n",
    "        feature_value = x[tree[\"feature\"]]\n",
    "        \n",
    "        # Si el valor no está en las ramas (dato no visto), usa la clase más común\n",
    "        if feature_value not in tree[\"branches\"]:\n",
    "            # Retorna la predicción del nodo actual (valor más común en entrenamiento)\n",
    "            return self._get_most_common_class(tree)\n",
    "        \n",
    "        return self._predict_sample(x, tree[\"branches\"][feature_value])\n",
    "    \n",
    "    def _get_most_common_class(self, tree):\n",
    "        \"\"\"Obtiene la clase más común en un subárbol\"\"\"\n",
    "        if tree[\"leaf\"]:\n",
    "            return tree[\"value\"]\n",
    "        # Recorre las ramas y encuentra la clase más común\n",
    "        classes = []\n",
    "        for branch in tree[\"branches\"].values():\n",
    "            classes.append(self._get_most_common_class(branch))\n",
    "        return Counter(classes).most_common(1)[0][0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predice las clases para un conjunto de datos\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Discretiza con el mismo discretizador del entrenamiento\n",
    "        X_discrete = self.discretizer_.transform(X)\n",
    "        \n",
    "        predictions = []\n",
    "        for x in X_discrete:\n",
    "            predictions.append(self._predict_sample(x, self.tree_))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\"\"):\n",
    "        \"\"\"Imprime el árbol de forma legible\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.tree_\n",
    "        \n",
    "        if tree[\"leaf\"]:\n",
    "            print(f\"{indent}└─ Predicción: {tree['value']} (n={tree['samples']})\")\n",
    "        else:\n",
    "            feature_name = tree[\"feature_name\"]\n",
    "            print(f\"{indent}├─ {feature_name} (n={tree['samples']})\")\n",
    "            for value, branch in tree[\"branches\"].items():\n",
    "                print(f\"{indent}│  └─ = {value}\")\n",
    "                self.print_tree(branch, indent + \"│     \")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRACIÓN CON TU CÓDIGO DE PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def train_id3_model(X_train, X_test, y_train, y_test, max_depth=10, n_bins=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo ID3 con tus datos preprocesados\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_train, X_test : array-like o DataFrame\n",
    "        Datos de entrenamiento y prueba\n",
    "    y_train, y_test : array-like o Series\n",
    "        Etiquetas de entrenamiento y prueba\n",
    "    max_depth : int, default=10\n",
    "        Profundidad máxima del árbol\n",
    "    n_bins : int, default=5\n",
    "        Número de bins para discretización\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Entrenando ID3 Decision Tree (max_depth={max_depth}, n_bins={n_bins})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape X_train: {X_train.shape}\")\n",
    "    print(f\"Shape y_train: {y_train.shape}\")\n",
    "    print(f\"Clases únicas: {len(np.unique(y_train))}\")\n",
    "    \n",
    "    # Crea y entrena el modelo\n",
    "    start_time = time.time()\n",
    "    id3 = ID3DecisionTree(max_depth=max_depth, min_samples_split=10, n_bins=n_bins)\n",
    "    \n",
    "    print(\"\\n[INFO] Iniciando fit()...\")\n",
    "    id3.fit(X_train, y_train)\n",
    "    fit_time = time.time() - start_time\n",
    "    print(f\"[INFO] Fit completado en {fit_time:.2f} segundos\")\n",
    "    \n",
    "    # Evaluación\n",
    "    print(\"\\n[INFO] Prediciendo en Train...\")\n",
    "    start_pred = time.time()\n",
    "    y_pred_train = id3.predict(X_train)\n",
    "    print(f\"[INFO] Predicción Train completada en {time.time() - start_pred:.2f} segundos\")\n",
    "    \n",
    "    print(\"[INFO] Prediciendo en Test...\")\n",
    "    start_pred = time.time()\n",
    "    y_pred_test = id3.predict(X_test)\n",
    "    print(f\"[INFO] Predicción Test completada en {time.time() - start_pred:.2f} segundos\")\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTADOS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy en Train: {train_acc:.4f}\")\n",
    "    print(f\"Accuracy en Test:  {test_acc:.4f}\")\n",
    "    print(f\"\\nReporte de clasificación (Test):\")\n",
    "    print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "    print(f\"\\nMatriz de confusión (Test):\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    \n",
    "    return id3\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EJEMPLO DE USO CON TU CÓDIGO\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "# Después de tu preprocessing, simplemente llama:\n",
    "\n",
    "# Sin transformación (ID3 hará la discretización internamente)\n",
    "id3_model = train_id3_model(X_train, X_test, y_train, y_test, max_depth=8, n_bins=5)\n",
    "\n",
    "# Para ver el árbol (cuidado si es muy grande):\n",
    "# id3_model.print_tree()\n",
    "\n",
    "# O si quieres usar evaluate_model de tu código:\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Crea un pipeline vacío (ID3 no necesita preprocessing adicional)\n",
    "id3_pipeline = Pipeline([\n",
    "    ('id3', ID3DecisionTree(max_depth=8, min_samples_split=10, n_bins=5))\n",
    "])\n",
    "\n",
    "id3_pipeline.fit(X_train, y_train)\n",
    "evaluate_model(\"ID3 Decision Tree\", id3_pipeline, X_test, y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de318de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando datos antes de ID3:\n",
      "X_train shape: (40000, 14)\n",
      "y_train shape: (40000,)\n",
      "Primeras 5 muestras de y_train: 823814    Comedy\n",
      "189120    Action\n",
      "752754     Drama\n",
      "438578    Comedy\n",
      "345993    Comedy\n",
      "Name: Genre, dtype: object\n",
      "\n",
      "============================================================\n",
      "Entrenando ID3 Decision Tree (max_depth=5, n_bins=3)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'USA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimeras 5 muestras de y_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Ejecuta el modelo\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m resultado \u001b[38;5;241m=\u001b[39m train_id3_model(X_train, X_test, y_train, y_test, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo entrenado:\u001b[39m\u001b[38;5;124m\"\u001b[39m, resultado)\n",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m, in \u001b[0;36mtrain_id3_model\u001b[1;34m(X_train, X_test, y_train, y_test, max_depth, n_bins)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Crea y entrena el modelo\u001b[39;00m\n\u001b[0;32m     21\u001b[0m id3 \u001b[38;5;241m=\u001b[39m ID3DecisionTree(max_depth\u001b[38;5;241m=\u001b[39mmax_depth, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_bins\u001b[38;5;241m=\u001b[39mn_bins)\n\u001b[1;32m---> 22\u001b[0m id3\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluación\u001b[39;00m\n\u001b[0;32m     25\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m id3\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "Cell \u001b[1;32mIn[32], line 135\u001b[0m, in \u001b[0;36mID3DecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Discretiza variables numéricas\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer_ \u001b[38;5;241m=\u001b[39m KBinsDiscretizer(\n\u001b[0;32m    130\u001b[0m     n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins, \n\u001b[0;32m    131\u001b[0m     encode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordinal\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    132\u001b[0m     strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantile\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    133\u001b[0m     subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    134\u001b[0m )\n\u001b[1;32m--> 135\u001b[0m X_discrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer_\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Construye el árbol\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_discrete, y)\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:231\u001b[0m, in \u001b[0;36mKBinsDiscretizer.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    Fit the estimator.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m    234\u001b[0m         output_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\julen.anda\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'USA'"
     ]
    }
   ],
   "source": [
    "def train_id3_model(X_train, X_test, y_train, y_test, max_depth=10, n_bins=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo ID3 con tus datos preprocesados\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    X_train, X_test : array-like o DataFrame\n",
    "        Datos de entrenamiento y prueba\n",
    "    y_train, y_test : array-like o Series\n",
    "        Etiquetas de entrenamiento y prueba\n",
    "    max_depth : int, default=10\n",
    "        Profundidad máxima del árbol\n",
    "    n_bins : int, default=5\n",
    "        Número de bins para discretización\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Entrenando ID3 Decision Tree (max_depth={max_depth}, n_bins={n_bins})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Crea y entrena el modelo\n",
    "    id3 = ID3DecisionTree(max_depth=max_depth, min_samples_split=10, n_bins=n_bins)\n",
    "    id3.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluación\n",
    "    y_pred_train = id3.predict(X_train)\n",
    "    y_pred_test = id3.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nAccuracy en Train: {train_acc:.4f}\")\n",
    "    print(f\"Accuracy en Test:  {test_acc:.4f}\")\n",
    "    print(f\"\\nReporte de clasificación (Test):\")\n",
    "    print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "    \n",
    "    return id3\n",
    "\n",
    "# Después de tu preprocessing\n",
    "print(\"Verificando datos antes de ID3:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"Primeras 5 muestras de y_train: {y_train.head()}\")\n",
    "\n",
    "# Ejecuta el modelo\n",
    "resultado = train_id3_model(X_train, X_test, y_train, y_test, max_depth=5, n_bins=3)\n",
    "print(\"Modelo entrenado:\", resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd2c48",
   "metadata": {},
   "source": [
    "CART (Criterion = gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3af988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [Decision Trees - CART (sklearn) - tuned and diagnostics]\n",
    "# Expandimos el grid para incluir criterio, balanceo de clases y max_features\n",
    "cart_pipe = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "grid_cart = {\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__max_depth\": [None, 8, 12, 20],\n",
    "    \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 3, 5],\n",
    "    \"clf__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"clf__ccp_alpha\": [0.0, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "scoring = {\"f1_macro\": \"f1_macro\", \"accuracy\": \"accuracy\"}\n",
    "# Usamos más folds para una estimación más estable (si no quieres más tiempo, vuelve a N_SPLITS)\n",
    "cv_cart = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs_cart = GridSearchCV(\n",
    "    estimator=cart_pipe, param_grid=grid_cart,\n",
    "    scoring=scoring, refit=\"f1_macro\", cv=cv_cart, n_jobs=-1\n",
    ")\n",
    "gs_cart.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n[CART] mejores params:\", gs_cart.best_params_)\n",
    "print(\"[CART] CV mean scores:\", {m: gs_cart.cv_results_[f\"mean_test_{m}\"][gs_cart.best_index_] for m in scoring})\n",
    "\n",
    "# Evaluación CART (tuneado)\n",
    "best_cart = gs_cart.best_estimator_\n",
    "res_cart = evaluate_model(\"CART (sklearn) - tuned\", best_cart, X_test, y_test)\n",
    "\n",
    "# Mostrar importancias de variables (si es posible extraer nombres tras el ColumnTransformer)\n",
    "try:\n",
    "    prep = best_cart.named_steps['prep']\n",
    "    # intentamos obtener nombres de features de forma robusta\n",
    "    try:\n",
    "        feature_names = prep.get_feature_names_out()\n",
    "    except Exception:\n",
    "        # Fallback: reconstruir nombres a partir de los transformadores\n",
    "        num_names = features_num\n",
    "        ohe = prep.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_names = list(ohe.get_feature_names_out(features_cat))\n",
    "        feature_names = list(num_names) + cat_names\n",
    "    importances = best_cart.named_steps['clf'].feature_importances_\n",
    "    fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    print(\"\\nTop 15 feature importances (CART tuned):\")\n",
    "    print(fi.head(15))\n",
    "except Exception as e:\n",
    "    print(\"No se pudieron extraer importancias de features:\", e)\n",
    "\n",
    "# --- (Opcional) Entrenar un RandomForest rápido como referencia ---\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    print(\"\\n[INFO] Entrenando RandomForest de referencia (200 estimators, class_weight='balanced')...\")\n",
    "    rf_pipe = Pipeline(steps=[\n",
    "        (\"prep\", prep_std),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1))\n",
    "    ])\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "    res_rf = evaluate_model(\"Random Forest (200)\", rf_pipe, X_test, y_test)\n",
    "except Exception as e:\n",
    "    print(\"No se entrenó RandomForest de referencia (posible falta de recursos):\", e)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Quick/cheap CART tuning: RandomizedSearchCV sobre una submuestra\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import math\n",
    "\n",
    "print(\"\\n[FAST CART] Búsqueda rápida (RandomizedSearchCV) sobre submuestra estratificada...\")\n",
    "# Elegimos un tamaño pequeño por clase para que sea rápido (p. ej. 200 por clase o menos)\n",
    "n_per_class_quick = min(200, max(20, int(X_train.shape[0] // (10 * y_train.nunique()))))\n",
    "X_quick, y_quick = stratified_sample(X_train, y_train, n_per_class_quick)\n",
    "print(f\"[FAST CART] Submuestra: {X_quick.shape[0]} filas ({n_per_class_quick} por clase aprox.)\")\n",
    "\n",
    "quick_pipe = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__max_depth\": [None, 8, 12],\n",
    "    \"clf__min_samples_split\": [2, 5, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 2, 3],\n",
    "    \"clf__max_features\": [None, \"sqrt\"],\n",
    "    \"clf__ccp_alpha\": [0.0, 1e-3]\n",
    "}\n",
    "\n",
    "# RandomizedSearch rápido: pocas iteraciones, cv=3 para velocidad\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=quick_pipe, param_distributions=param_dist,\n",
    "    n_iter=24, scoring=scoring, refit=\"f1_macro\", cv=3, n_jobs=-1, random_state=RANDOM_STATE\n",
    ")\n",
    "rs.fit(X_quick, y_quick)\n",
    "\n",
    "print(\"\\n[FAST CART] mejores params (rápido):\", rs.best_params_)\n",
    "print(\"[FAST CART] CV mean scores (rápido):\", {m: rs.cv_results_[f\"mean_test_{m}\"][rs.best_index_] for m in scoring})\n",
    "\n",
    "best_cart_fast = rs.best_estimator_\n",
    "res_cart_fast = evaluate_model(\"CART (fast-randomized)\", best_cart_fast, X_test, y_test)\n",
    "\n",
    "print(\"\\n[FAST CART] Si este resultado es razonable, puedes usar los params como inicio o reducir el grid para GridSearch final.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5c9c7",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes\n",
    "\n",
    "Train and evaluate a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4590f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NB-Mult] mejores params: {'clf__alpha': 0.0}\n",
      "[NB-Mult] CV mean scores: {'f1_macro': 0.09664652087151866, 'accuracy': 0.12437502140493366}\n",
      "\n",
      "== Naive Bayes (Gaussian) ==\n",
      "accuracy           0.129400\n",
      "precision_macro    0.136299\n",
      "recall_macro       0.129400\n",
      "f1_macro           0.081846\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[141  69  41  15  46 873  18  47]\n",
      " [131  87  48   9  61 858  16  40]\n",
      " [127  73  46   8  53 893  16  34]\n",
      " [125  88  44  19  45 878  17  34]\n",
      " [130  80  48  11  52 853  28  48]\n",
      " [119  64  47   9  50 892  19  50]\n",
      " [136  69  47   9  55 884  12  38]\n",
      " [136  70  32  12  56 889  10  45]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.13      0.11      0.12      1250\n",
      "      Comedy       0.14      0.07      0.09      1250\n",
      " Documentary       0.13      0.04      0.06      1250\n",
      "       Drama       0.21      0.02      0.03      1250\n",
      "      Horror       0.12      0.04      0.06      1250\n",
      "     Romance       0.13      0.71      0.22      1250\n",
      "      Sci-Fi       0.09      0.01      0.02      1250\n",
      "    Thriller       0.13      0.04      0.06      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.14      0.13      0.08     10000\n",
      "weighted avg       0.14      0.13      0.08     10000\n",
      "\n",
      "\n",
      "== Naive Bayes (Multinomial) ==\n",
      "accuracy           0.125300\n",
      "precision_macro    0.111565\n",
      "recall_macro       0.125300\n",
      "f1_macro           0.097355\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[113  95 656  54 129 159   0  44]\n",
      " [ 99 105 656  49 148 151   0  42]\n",
      " [102 121 623  60 135 168   1  40]\n",
      " [101 114 646  57 128 165   0  39]\n",
      " [104 106 639  59 166 138   0  38]\n",
      " [ 95 103 643  71 150 150   0  38]\n",
      " [106 102 664  60 134 149   0  35]\n",
      " [104  96 625  64 142 180   0  39]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.14      0.09      0.11      1250\n",
      "      Comedy       0.12      0.08      0.10      1250\n",
      " Documentary       0.12      0.50      0.19      1250\n",
      "       Drama       0.12      0.05      0.07      1250\n",
      "      Horror       0.15      0.13      0.14      1250\n",
      "     Romance       0.12      0.12      0.12      1250\n",
      "      Sci-Fi       0.00      0.00      0.00      1250\n",
      "    Thriller       0.12      0.03      0.05      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.11      0.13      0.10     10000\n",
      "weighted avg       0.11      0.13      0.10     10000\n",
      "\n",
      "\n",
      "[NB] Mejor variante: Naive Bayes (Multinomial)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GaussianNB (para continuas)\n",
    "nb_gaussian = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", GaussianNB())\n",
    "])\n",
    "\n",
    "# MultinomialNB (requiere no-negatividad; MinMax ayuda)\n",
    "nb_mult = Pipeline(steps=[\n",
    "    (\"prep\", prep_minmax),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Pequeño grid para Multinomial\n",
    "grid_nb_mult = {\"clf__alpha\": [0.0, 0.5, 1.0]}\n",
    "\n",
    "gs_nb_mult = GridSearchCV(\n",
    "    estimator=nb_mult, param_grid=grid_nb_mult,\n",
    "    scoring=scoring, refit=\"f1_macro\", cv=cv, n_jobs=-1\n",
    ")\n",
    "gs_nb_mult.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n[NB-Mult] mejores params:\", gs_nb_mult.best_params_)\n",
    "print(\"[NB-Mult] CV mean scores:\", {m: gs_nb_mult.cv_results_[f\"mean_test_{m}\"][gs_nb_mult.best_index_] for m in scoring})\n",
    "\n",
    "# Entrena GaussianNB (sin grid)\n",
    "nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "# Evalúa ambos y elige mejor por F1 macro\n",
    "res_nbG = evaluate_model(\"Naive Bayes (Gaussian)\", nb_gaussian, X_test, y_test)\n",
    "res_nbM = evaluate_model(\"Naive Bayes (Multinomial)\", gs_nb_mult.best_estimator_, X_test, y_test)\n",
    "\n",
    "best_nb_res = res_nbM if res_nbM[\"f1_macro\"] >= res_nbG[\"f1_macro\"] else res_nbG\n",
    "best_nb_name = best_nb_res[\"model\"]\n",
    "print(f\"\\n[NB] Mejor variante: {best_nb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73925891",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machine (SVM)\n",
    "\n",
    "Train and evaluate a Support Vector Machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b16005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SVM] mejores params: {'clf__C': 10, 'clf__gamma': 0.1}\n",
      "[SVM] CV mean scores: {'f1_macro': 0.12805549562594667, 'accuracy': 0.12899990314015652}\n",
      "\n",
      "== SVM (RBF) ==\n",
      "accuracy           0.124400\n",
      "precision_macro    0.124997\n",
      "recall_macro       0.124400\n",
      "f1_macro           0.123396\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[145 168 219 151 141 161 131 134]\n",
      " [153 151 252 134 130 183 124 123]\n",
      " [179 164 228 145 114 190 111 119]\n",
      " [158 175 219 138 119 171 136 134]\n",
      " [173 161 234 141 148 171 102 120]\n",
      " [158 150 243 144 130 178 112 135]\n",
      " [153 162 220 156 122 181 116 140]\n",
      " [169 156 233 145 113 158 136 140]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.11      0.12      0.11      1250\n",
      "      Comedy       0.12      0.12      0.12      1250\n",
      " Documentary       0.12      0.18      0.15      1250\n",
      "       Drama       0.12      0.11      0.11      1250\n",
      "      Horror       0.15      0.12      0.13      1250\n",
      "     Romance       0.13      0.14      0.13      1250\n",
      "      Sci-Fi       0.12      0.09      0.10      1250\n",
      "    Thriller       0.13      0.11      0.12      1250\n",
      "\n",
      "    accuracy                           0.12     10000\n",
      "   macro avg       0.12      0.12      0.12     10000\n",
      "weighted avg       0.12      0.12      0.12     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SVM es costoso. Submuestreo adicional para entrenar con tiempos razonables\n",
    "n_per_class_svm = SAMPLE_SIZE_FOR_SVM // y_train.nunique()\n",
    "X_svm_train, y_svm_train = stratified_sample(X_train, y_train, n_per_class_svm)\n",
    "\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", SVC(kernel=\"rbf\", probability=False, class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "grid_svm = {\n",
    "    \"clf__C\": [1, 5, 10],\n",
    "    \"clf__gamma\": [\"scale\", 0.1]\n",
    "}\n",
    "\n",
    "gs_svm = GridSearchCV(\n",
    "    estimator=svm_pipe, param_grid=grid_svm,\n",
    "    scoring=scoring, refit=\"f1_macro\", cv=cv, n_jobs=-1\n",
    ")\n",
    "gs_svm.fit(X_svm_train, y_svm_train)\n",
    "\n",
    "print(\"\\n[SVM] mejores params:\", gs_svm.best_params_)\n",
    "print(\"[SVM] CV mean scores:\", {m: gs_svm.cv_results_[f\"mean_test_{m}\"][gs_svm.best_index_] for m in scoring})\n",
    "\n",
    "# Evaluación SVM en X_test completo (preprocesado dentro del pipeline)\n",
    "best_svm = gs_svm.best_estimator_\n",
    "res_svm = evaluate_model(\"SVM (RBF)\", best_svm, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ccfe1",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare the performance of all classifiers using accuracy and classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6facb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  Accuracy\n",
      "0   ID3 Decision Tree  0.158865\n",
      "1  CART Decision Tree  0.158345\n",
      "2         Naive Bayes  0.244365\n",
      "3                 SVM  0.248845\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %% [Comparativa final]\n",
    "results = [res_cart, best_nb_res, res_svm, res_id3]\n",
    "df_res = pd.DataFrame(results).set_index(\"model\").sort_values(\"f1_macro\", ascending=False)\n",
    "print(\"\\n=== COMPARATIVA (ordenado por F1_macro) ===\\n\", df_res)\n",
    "\n",
    "# Guardar resultados a CSV (opcional)\n",
    "df_res.to_csv(\"classification_results.csv\", index=True)\n",
    "\n",
    "# (Opcional) gráfico rápido de F1 macro\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=df_res.index, y=df_res[\"f1_macro\"], color=\"#1f77b4\")\n",
    "plt.title(\"Comparativa F1 Macro por modelo (test)\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
