{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66408313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [Imports]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f738e7",
   "metadata": {},
   "source": [
    "# Classification Analysis\n",
    "\n",
    "This notebook covers classification using Decision Trees (ID3, CART), Naive Bayes, and Support Vector Machine (SVM). Model evaluation and comparison are included for each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7387ffc",
   "metadata": {},
   "source": [
    "## 1. Data Loading, Preprocessing, and Exploration\n",
    "\n",
    "We will load the dataset, preprocess it to optimize classification performance, and visualize key aspects. Preprocessing includes handling missing values, encoding categorical variables, and scaling features. These steps are chosen to ensure models receive clean, numerical, and standardized data, which improves accuracy and comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ded7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración global\n",
    "RANDOM_STATE = 42\n",
    "rng = check_random_state(RANDOM_STATE)\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# Tamaños para trabajar de forma eficiente en máquinas normales\n",
    "TEST_SIZE = 0.2\n",
    "N_SPLITS = 3\n",
    "SAMPLE_SIZE_FOR_GRID = 50_000   # filas para tuning (GridSearch)\n",
    "SAMPLE_SIZE_FOR_SVM  = 20_000   # filas para entrenar SVM RBF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26abea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº muestras: 999999 | Nº features candidatas: 14\n",
      "Distribución de clases (top 8):\n",
      " Genre\n",
      "Drama          0.25\n",
      "Comedy         0.20\n",
      "Action         0.15\n",
      "Thriller       0.10\n",
      "Romance        0.10\n",
      "Horror         0.10\n",
      "Documentary    0.05\n",
      "Sci-Fi         0.05\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [Data loading, pre-processing & exploration]\n",
    "DATA_PATH = \"./data/movies_dataset.csv\"  # ajusta si es otra ruta/archivo\n",
    "TARGET = \"Genre\"\n",
    "\n",
    "# Carga\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Parseo de fecha (día-mes-año en tu CSV)\n",
    "df[\"ReleaseDate\"] = pd.to_datetime(df[\"ReleaseDate\"], dayfirst=True, errors=\"coerce\")\n",
    "df[\"ReleaseMonth\"]   = df[\"ReleaseDate\"].dt.month\n",
    "df[\"ReleaseDay\"]     = df[\"ReleaseDate\"].dt.day\n",
    "df[\"ReleaseWeekday\"] = df[\"ReleaseDate\"].dt.weekday\n",
    "\n",
    "# Selección de variables con baja/mediana cardinalidad (evitamos One-Hot gigante)\n",
    "features_num = [\n",
    "    \"ReleaseYear\", \"ReleaseMonth\", \"ReleaseDay\", \"ReleaseWeekday\",\n",
    "    \"BudgetUSD\", \"US_BoxOfficeUSD\", \"Global_BoxOfficeUSD\",\n",
    "    \"Opening_Day_SalesUSD\", \"One_Week_SalesUSD\",\n",
    "    \"IMDbRating\", \"RottenTomatoesScore\", \"NumVotesIMDb\", \"NumVotesRT\"\n",
    "]\n",
    "features_cat = [\"Country\"]  # One-Hot controlado\n",
    "\n",
    "X_full = df[features_num + features_cat].copy()\n",
    "y_full = df[TARGET].copy()\n",
    "\n",
    "# Exploración mínima\n",
    "print(f\"Nº muestras: {len(df)} | Nº features candidatas: {len(features_num) + len(features_cat)}\")\n",
    "print(\"Distribución de clases (top 8):\\n\", y_full.value_counts(normalize=True).round(3))\n",
    "\n",
    "# Utilidad: submuestreo estratificado por clase\n",
    "def stratified_sample(X, y, n_per_class):\n",
    "    parts = []\n",
    "    for cls in y.unique():\n",
    "        idx = y[y == cls].index\n",
    "        take = min(n_per_class, len(idx))\n",
    "        sampled = rng.choice(idx, size=take, replace=False)\n",
    "        parts.append(pd.DataFrame({\"i\": sampled}))\n",
    "    sel = pd.concat(parts)[\"i\"].values\n",
    "    return X.loc[sel], y.loc[sel]\n",
    "\n",
    "# Generamos un conjunto manejable para grids (aprox. 50k)\n",
    "n_per_class_grid = SAMPLE_SIZE_FOR_GRID // y_full.nunique()\n",
    "X_small, y_small = stratified_sample(X_full, y_full, n_per_class_grid)\n",
    "\n",
    "# Split train/test estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small, test_size=TEST_SIZE, stratify=y_small, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Preprocesadores: StandardScaler (num) + One-Hot (cat)\n",
    "numeric_std = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "numeric_minmax = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "categorical_oh = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "prep_std = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_std, features_num),\n",
    "        (\"cat\", categorical_oh, features_cat)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "prep_minmax = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_minmax, features_num),\n",
    "        (\"cat\", categorical_oh, features_cat)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Función de evaluación común\n",
    "def evaluate_model(nombre, modelo, X_te, y_te):\n",
    "    y_pred = modelo.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_te, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n== {nombre} ==\")\n",
    "    print(pd.Series({\"accuracy\": acc, \"precision_macro\": pr, \"recall_macro\": rc, \"f1_macro\": f1}))\n",
    "    print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_te, y_pred))\n",
    "    print(\"\\nReporte por clase:\\n\", classification_report(y_te, y_pred, zero_division=0))\n",
    "    return {\"model\": nombre, \"accuracy\": acc, \"precision_macro\": pr, \"recall_macro\": rc, \"f1_macro\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04aea68",
   "metadata": {},
   "source": [
    "## 3. Decision Trees (ID3 & CART)\n",
    "\n",
    "We will train and evaluate Decision Tree classifiers using both ID3 (entropy) and CART (gini) criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2c64d",
   "metadata": {},
   "source": [
    "ID3 (entropía)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08e61841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== ID3 (from-scratch) ==\n",
      "accuracy           0.128100\n",
      "precision_macro    0.128081\n",
      "recall_macro       0.128100\n",
      "f1_macro           0.128080\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[149 177 139 161 179 131 165 149]\n",
      " [146 167 143 164 149 149 167 165]\n",
      " [162 166 156 139 159 156 149 163]\n",
      " [133 142 153 184 156 152 167 163]\n",
      " [158 156 170 141 162 137 157 169]\n",
      " [163 156 150 159 173 157 142 150]\n",
      " [162 159 160 147 141 172 147 162]\n",
      " [163 152 144 159 155 171 147 159]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.12      0.12      0.12      1250\n",
      "      Comedy       0.13      0.13      0.13      1250\n",
      " Documentary       0.13      0.12      0.13      1250\n",
      "       Drama       0.15      0.15      0.15      1250\n",
      "      Horror       0.13      0.13      0.13      1250\n",
      "     Romance       0.13      0.13      0.13      1250\n",
      "      Sci-Fi       0.12      0.12      0.12      1250\n",
      "    Thriller       0.12      0.13      0.13      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.13      0.13      0.13     10000\n",
      "weighted avg       0.13      0.13      0.13     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [Decision Trees - ID3 (entropía)]\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def entropy(y):\n",
    "    counts = np.array(list(Counter(y).values()), dtype=float)\n",
    "    p = counts / counts.sum()\n",
    "    p = p[p > 0]\n",
    "    return -(p * np.log2(p)).sum()\n",
    "\n",
    "def information_gain(y, splits):\n",
    "    H_parent = entropy(y)\n",
    "    n = len(y)\n",
    "    H_children = 0.0\n",
    "    for idx in splits:\n",
    "        child = y[idx] if isinstance(idx, np.ndarray) else y.iloc[idx]\n",
    "        w = (child.size if hasattr(child, 'size') else len(child)) / n\n",
    "        if len(child) > 0:\n",
    "            H_children += w * entropy(child)\n",
    "    return H_parent - H_children\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature: str = None\n",
    "    threshold: float = None\n",
    "    branches: dict = None\n",
    "    left: 'Node' = None\n",
    "    right: 'Node' = None\n",
    "    label: any = None\n",
    "\n",
    "class ID3:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_gain=1e-9):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_gain = min_gain\n",
    "        self.tree_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X_df: pd.DataFrame, y: pd.Series):\n",
    "        X = X_df.copy()\n",
    "        # imputación simple\n",
    "        for c in X.columns:\n",
    "            if pd.api.types.is_numeric_dtype(X[c]):\n",
    "                X[c] = X[c].fillna(X[c].median())\n",
    "            else:\n",
    "                X[c] = X[c].fillna(X[c].mode().iloc[0]).astype(\"category\")\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.tree_ = self._grow(X, y, 0)\n",
    "        return self\n",
    "\n",
    "    def _majority(self, y): return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def _best_split_numeric(self, x, y):\n",
    "        order = np.argsort(x)\n",
    "        x_sorted, y_sorted = x[order], y[order]\n",
    "        uniq = np.unique(x_sorted)\n",
    "        if uniq.size <= 1: return None, -np.inf\n",
    "        thr_candidates = (uniq[:-1] + uniq[1:]) / 2.0\n",
    "        best_gain, best_thr = -np.inf, None\n",
    "        for thr in thr_candidates:\n",
    "            left_mask = x <= thr\n",
    "            right_mask = ~left_mask\n",
    "            gain = information_gain(y, [left_mask, right_mask])\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_thr = gain, thr\n",
    "        return best_thr, best_gain\n",
    "\n",
    "    def _grow(self, X, y, depth):\n",
    "        if len(np.unique(y)) == 1: return Node(label=y.iloc[0])\n",
    "        if self.max_depth is not None and depth >= self.max_depth: return Node(label=self._majority(y))\n",
    "        if len(X) < self.min_samples_split or X.shape[1] == 0: return Node(label=self._majority(y))\n",
    "\n",
    "        best_feature, best_gain, best_split, best_kind = None, -np.inf, None, None\n",
    "        for feat in X.columns:\n",
    "            col = X[feat]\n",
    "            if pd.api.types.is_numeric_dtype(col):\n",
    "                thr, gain = self._best_split_numeric(col.values, y.values)\n",
    "                if thr is not None and gain > best_gain:\n",
    "                    best_feature, best_gain, best_split, best_kind = feat, gain, thr, \"num\"\n",
    "            else:\n",
    "                cats = col.astype(\"category\").cat.categories\n",
    "                splits = [(col == cat).values for cat in cats]\n",
    "                gain = information_gain(y.values, splits)\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_gain, best_split, best_kind = feat, gain, cats, \"cat\"\n",
    "\n",
    "        if best_gain < self.min_gain or best_feature is None:\n",
    "            return Node(label=self._majority(y))\n",
    "\n",
    "        if best_kind == \"num\":\n",
    "            thr = best_split\n",
    "            node = Node(feature=best_feature, threshold=thr)\n",
    "            left_mask = X[best_feature] <= thr\n",
    "            right_mask = ~left_mask\n",
    "            node.left  = self._grow(X[left_mask],  y[left_mask],  depth+1)\n",
    "            node.right = self._grow(X[right_mask], y[right_mask], depth+1)\n",
    "            return node\n",
    "        else:\n",
    "            cats = best_split\n",
    "            node = Node(feature=best_feature, branches={})\n",
    "            for cat in cats:\n",
    "                mask = X[best_feature] == cat\n",
    "                node.branches[cat] = self._grow(X[mask], y[mask], depth+1) if mask.sum() > 0 else Node(label=self._majority(y))\n",
    "            return node\n",
    "\n",
    "    def _predict_row(self, row, node: Node):\n",
    "        while node.label is None:\n",
    "            val = row.get(node.feature, None)\n",
    "            if node.threshold is not None:\n",
    "                node = node.left if val <= node.threshold else node.right\n",
    "            else:\n",
    "                node = node.branches.get(val, Node(label=self.classes_[0]))\n",
    "        return node.label\n",
    "\n",
    "    def predict(self, X_df: pd.DataFrame):\n",
    "        return np.array([self._predict_row(row, self.tree_) for _, row in X_df.iterrows()])\n",
    "\n",
    "# Entrenar ID3 sobre X_train (ya preprocesado simple en fit)\n",
    "id3 = ID3(max_depth=None, min_samples_split=2, min_gain=1e-9)\n",
    "id3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación ID3\n",
    "res_id3 = evaluate_model(\"ID3 (from-scratch)\", id3, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd2c48",
   "metadata": {},
   "source": [
    "CART (Criterion = gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33cbb221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CART] mejores params: {'clf__ccp_alpha': 0.0, 'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}\n",
      "[CART] CV mean scores: {'f1_macro': 0.12599193916318224, 'accuracy': 0.12629994578423032}\n",
      "\n",
      "== CART (sklearn) ==\n",
      "accuracy           0.126200\n",
      "precision_macro    0.126566\n",
      "recall_macro       0.126200\n",
      "f1_macro           0.126003\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[179 195 165 144 139 145 139 144]\n",
      " [193 182 157 150 132 151 154 131]\n",
      " [196 158 149 171 147 157 144 128]\n",
      " [204 176 174 156 135 146 135 124]\n",
      " [189 164 169 173 157 141 139 118]\n",
      " [163 157 171 180 141 158 150 130]\n",
      " [190 158 172 152 148 150 153 127]\n",
      " [194 173 165 148 148 140 154 128]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.12      0.14      0.13      1250\n",
      "      Comedy       0.13      0.15      0.14      1250\n",
      " Documentary       0.11      0.12      0.12      1250\n",
      "       Drama       0.12      0.12      0.12      1250\n",
      "      Horror       0.14      0.13      0.13      1250\n",
      "     Romance       0.13      0.13      0.13      1250\n",
      "      Sci-Fi       0.13      0.12      0.13      1250\n",
      "    Thriller       0.12      0.10      0.11      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.13      0.13      0.13     10000\n",
      "weighted avg       0.13      0.13      0.13     10000\n",
      "\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.12      0.14      0.13      1250\n",
      "      Comedy       0.13      0.15      0.14      1250\n",
      " Documentary       0.11      0.12      0.12      1250\n",
      "       Drama       0.12      0.12      0.12      1250\n",
      "      Horror       0.14      0.13      0.13      1250\n",
      "     Romance       0.13      0.13      0.13      1250\n",
      "      Sci-Fi       0.13      0.12      0.13      1250\n",
      "    Thriller       0.12      0.10      0.11      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.13      0.13      0.13     10000\n",
      "weighted avg       0.13      0.13      0.13     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [Decision Trees - CART (sklearn)]\n",
    "cart_pipe = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", DecisionTreeClassifier(criterion=\"gini\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "grid_cart = {\n",
    "    \"clf__max_depth\": [None, 8, 12],\n",
    "    \"clf__min_samples_split\": [2, 10],\n",
    "    \"clf__min_samples_leaf\": [1, 3],\n",
    "    \"clf__ccp_alpha\": [0.0, 0.001]\n",
    "}\n",
    "\n",
    "scoring = {\"f1_macro\": \"f1_macro\", \"accuracy\": \"accuracy\"}\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "gs_cart = GridSearchCV(\n",
    "    estimator=cart_pipe, param_grid=grid_cart,\n",
    "    scoring=scoring, refit=\"f1_macro\", cv=cv, n_jobs=-1\n",
    ")\n",
    "gs_cart.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n[CART] mejores params:\", gs_cart.best_params_)\n",
    "print(\"[CART] CV mean scores:\", {m: gs_cart.cv_results_[f\"mean_test_{m}\"][gs_cart.best_index_] for m in scoring})\n",
    "\n",
    "# Evaluación CART\n",
    "best_cart = gs_cart.best_estimator_\n",
    "res_cart = evaluate_model(\"CART (sklearn)\", best_cart, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5c9c7",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes\n",
    "\n",
    "Train and evaluate a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4590f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NB-Mult] mejores params: {'clf__alpha': 0.0}\n",
      "[NB-Mult] CV mean scores: {'f1_macro': 0.09664652087151866, 'accuracy': 0.12437502140493366}\n",
      "\n",
      "== Naive Bayes (Gaussian) ==\n",
      "accuracy           0.129400\n",
      "precision_macro    0.136299\n",
      "recall_macro       0.129400\n",
      "f1_macro           0.081846\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[141  69  41  15  46 873  18  47]\n",
      " [131  87  48   9  61 858  16  40]\n",
      " [127  73  46   8  53 893  16  34]\n",
      " [125  88  44  19  45 878  17  34]\n",
      " [130  80  48  11  52 853  28  48]\n",
      " [119  64  47   9  50 892  19  50]\n",
      " [136  69  47   9  55 884  12  38]\n",
      " [136  70  32  12  56 889  10  45]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.13      0.11      0.12      1250\n",
      "      Comedy       0.14      0.07      0.09      1250\n",
      " Documentary       0.13      0.04      0.06      1250\n",
      "       Drama       0.21      0.02      0.03      1250\n",
      "      Horror       0.12      0.04      0.06      1250\n",
      "     Romance       0.13      0.71      0.22      1250\n",
      "      Sci-Fi       0.09      0.01      0.02      1250\n",
      "    Thriller       0.13      0.04      0.06      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.14      0.13      0.08     10000\n",
      "weighted avg       0.14      0.13      0.08     10000\n",
      "\n",
      "\n",
      "== Naive Bayes (Multinomial) ==\n",
      "accuracy           0.125300\n",
      "precision_macro    0.111565\n",
      "recall_macro       0.125300\n",
      "f1_macro           0.097355\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[113  95 656  54 129 159   0  44]\n",
      " [ 99 105 656  49 148 151   0  42]\n",
      " [102 121 623  60 135 168   1  40]\n",
      " [101 114 646  57 128 165   0  39]\n",
      " [104 106 639  59 166 138   0  38]\n",
      " [ 95 103 643  71 150 150   0  38]\n",
      " [106 102 664  60 134 149   0  35]\n",
      " [104  96 625  64 142 180   0  39]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.14      0.09      0.11      1250\n",
      "      Comedy       0.12      0.08      0.10      1250\n",
      " Documentary       0.12      0.50      0.19      1250\n",
      "       Drama       0.12      0.05      0.07      1250\n",
      "      Horror       0.15      0.13      0.14      1250\n",
      "     Romance       0.12      0.12      0.12      1250\n",
      "      Sci-Fi       0.00      0.00      0.00      1250\n",
      "    Thriller       0.12      0.03      0.05      1250\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.11      0.13      0.10     10000\n",
      "weighted avg       0.11      0.13      0.10     10000\n",
      "\n",
      "\n",
      "[NB] Mejor variante: Naive Bayes (Multinomial)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GaussianNB (para continuas)\n",
    "nb_gaussian = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", GaussianNB())\n",
    "])\n",
    "\n",
    "# MultinomialNB (requiere no-negatividad; MinMax ayuda)\n",
    "nb_mult = Pipeline(steps=[\n",
    "    (\"prep\", prep_minmax),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Pequeño grid para Multinomial\n",
    "grid_nb_mult = {\"clf__alpha\": [0.0, 0.5, 1.0]}\n",
    "\n",
    "gs_nb_mult = GridSearchCV(\n",
    "    estimator=nb_mult, param_grid=grid_nb_mult,\n",
    "    scoring=scoring, refit=\"f1_macro\", cv=cv, n_jobs=-1\n",
    ")\n",
    "gs_nb_mult.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n[NB-Mult] mejores params:\", gs_nb_mult.best_params_)\n",
    "print(\"[NB-Mult] CV mean scores:\", {m: gs_nb_mult.cv_results_[f\"mean_test_{m}\"][gs_nb_mult.best_index_] for m in scoring})\n",
    "\n",
    "# Entrena GaussianNB (sin grid)\n",
    "nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "# Evalúa ambos y elige mejor por F1 macro\n",
    "res_nbG = evaluate_model(\"Naive Bayes (Gaussian)\", nb_gaussian, X_test, y_test)\n",
    "res_nbM = evaluate_model(\"Naive Bayes (Multinomial)\", gs_nb_mult.best_estimator_, X_test, y_test)\n",
    "\n",
    "best_nb_res = res_nbM if res_nbM[\"f1_macro\"] >= res_nbG[\"f1_macro\"] else res_nbG\n",
    "best_nb_name = best_nb_res[\"model\"]\n",
    "print(f\"\\n[NB] Mejor variante: {best_nb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73925891",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machine (SVM)\n",
    "\n",
    "Train and evaluate a Support Vector Machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b16005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SVM] mejores params: {'clf__C': 10, 'clf__gamma': 0.1}\n",
      "[SVM] CV mean scores: {'f1_macro': 0.12805549562594667, 'accuracy': 0.12899990314015652}\n",
      "\n",
      "== SVM (RBF) ==\n",
      "accuracy           0.124400\n",
      "precision_macro    0.124997\n",
      "recall_macro       0.124400\n",
      "f1_macro           0.123396\n",
      "dtype: float64\n",
      "\n",
      "Matriz de confusión:\n",
      " [[145 168 219 151 141 161 131 134]\n",
      " [153 151 252 134 130 183 124 123]\n",
      " [179 164 228 145 114 190 111 119]\n",
      " [158 175 219 138 119 171 136 134]\n",
      " [173 161 234 141 148 171 102 120]\n",
      " [158 150 243 144 130 178 112 135]\n",
      " [153 162 220 156 122 181 116 140]\n",
      " [169 156 233 145 113 158 136 140]]\n",
      "\n",
      "Reporte por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.11      0.12      0.11      1250\n",
      "      Comedy       0.12      0.12      0.12      1250\n",
      " Documentary       0.12      0.18      0.15      1250\n",
      "       Drama       0.12      0.11      0.11      1250\n",
      "      Horror       0.15      0.12      0.13      1250\n",
      "     Romance       0.13      0.14      0.13      1250\n",
      "      Sci-Fi       0.12      0.09      0.10      1250\n",
      "    Thriller       0.13      0.11      0.12      1250\n",
      "\n",
      "    accuracy                           0.12     10000\n",
      "   macro avg       0.12      0.12      0.12     10000\n",
      "weighted avg       0.12      0.12      0.12     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SVM es costoso. Submuestreo adicional para entrenar con tiempos razonables\n",
    "n_per_class_svm = SAMPLE_SIZE_FOR_SVM // y_train.nunique()\n",
    "X_svm_train, y_svm_train = stratified_sample(X_train, y_train, n_per_class_svm)\n",
    "\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"prep\", prep_std),\n",
    "    (\"clf\", SVC(kernel=\"rbf\", probability=False, class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "grid_svm = {\n",
    "    \"clf__C\": [1, 5, 10],\n",
    "    \"clf__gamma\": [\"scale\", 0.1]\n",
    "}\n",
    "\n",
    "gs_svm = GridSearchCV(\n",
    "    estimator=svm_pipe, param_grid=grid_svm,\n",
    "    scoring=scoring, refit=\"f1_macro\", cv=cv, n_jobs=-1\n",
    ")\n",
    "gs_svm.fit(X_svm_train, y_svm_train)\n",
    "\n",
    "print(\"\\n[SVM] mejores params:\", gs_svm.best_params_)\n",
    "print(\"[SVM] CV mean scores:\", {m: gs_svm.cv_results_[f\"mean_test_{m}\"][gs_svm.best_index_] for m in scoring})\n",
    "\n",
    "# Evaluación SVM en X_test completo (preprocesado dentro del pipeline)\n",
    "best_svm = gs_svm.best_estimator_\n",
    "res_svm = evaluate_model(\"SVM (RBF)\", best_svm, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ccfe1",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare the performance of all classifiers using accuracy and classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6facb8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_id3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %% [Comparativa final]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m [res_cart, best_nb_res, res_svm, res_id3]\n\u001b[0;32m      3\u001b[0m df_res \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== COMPARATIVA (ordenado por F1_macro) ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, df_res)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_id3' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [Comparativa final]\n",
    "results = [res_cart, best_nb_res, res_svm, res_id3]\n",
    "df_res = pd.DataFrame(results).set_index(\"model\").sort_values(\"f1_macro\", ascending=False)\n",
    "print(\"\\n=== COMPARATIVA (ordenado por F1_macro) ===\\n\", df_res)\n",
    "\n",
    "# Guardar resultados a CSV (opcional)\n",
    "df_res.to_csv(\"classification_results.csv\", index=True)\n",
    "\n",
    "# (Opcional) gráfico rápido de F1 macro\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=df_res.index, y=df_res[\"f1_macro\"], color=\"#1f77b4\")\n",
    "plt.title(\"Comparativa F1 Macro por modelo (test)\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
